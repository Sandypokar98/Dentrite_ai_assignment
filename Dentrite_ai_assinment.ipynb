{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f73142-b706-4a08-801b-09eede7cb063",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f97c22fa-09d2-495c-98e4-d55e32a0dcae",
   "metadata": {},
   "source": [
    "#requirement.txt\n",
    "pandas     #2.2.2\n",
    "numpy      #1.26.4\n",
    "json       #2.0.9\n",
    "striprtf   #0.0.29\n",
    "sklearn    #1.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "36ac035b-bafa-4d7a-8119-09a67cd28102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import striprtf\n",
    "import sklearn\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression,SGDRegressor,Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b0112-d6cd-4423-9695-3119f52034d7",
   "metadata": {},
   "source": [
    "# Parsing rtf to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c4ad28c1-95ef-4e7d-a072-1367316a135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening .rtf file in read mode. read and convert rtf to text using rtf_to_text method from striprtf library\n",
    "with open(\"algoparams_from_ui.json.rtf\",\"r\") as f1:\n",
    "    rtf_str=f1.read()\n",
    "    text=rtf_to_text(rtf_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "36540172-d0a1-4773-9114-f830dfdad1f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#converted text contains \\n in text foemat we need \"new line\" instead of \\n\n",
    "#so we write the converted text in text file to remove \\n from text\n",
    "with open(\"j_file.txt\",\"w\") as f2:\n",
    "    f2.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8096119f-d810-475f-9b94-4f20db1d1c0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loading text data in json format and we have a dictionary object json_data\n",
    "with open(\"j_file.txt\",\"r\") as f3:\n",
    "    json_data=json.load(f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "018ba6d7-b62d-4b75-a97c-6ad3f54631e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#making json file\n",
    "with open(\"j_file.json\",\"w\") as f4:\n",
    "    json.dump(json_data,f4,indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e13c10-d459-4064-9880-d5db5e82f919",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Reading target and type of regression to be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6d6cfa41-dac8-40d3-b77a-8147124eddba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"j_file.json\",\"r\") as f1:\n",
    "    json_data=json.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "79974280-b2fd-435e-81e7-ac284e014412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width      species\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading target configuration\n",
    "target=json_data[\"design_state_data\"][\"target\"]\n",
    "#target column\n",
    "target_column=json_data[\"design_state_data\"][\"target\"][\"target\"]\n",
    "#prediction type\n",
    "pred_type=json_data[\"design_state_data\"][\"target\"][\"prediction_type\"]\n",
    "#model type\n",
    "mod_type=json_data[\"design_state_data\"][\"target\"][\"type\"]\n",
    "#feature handling configuration\n",
    "feature_handling=json_data[\"design_state_data\"][\"feature_handling\"]\n",
    "#reading data\n",
    "df=pd.read_csv(\"iris.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab3594d-e3ab-4f72-8a71-03f83c3ba6a3",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Feature Handling, Application of Imputation on Iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "768a9bee-29f5-45d5-84a9-7664cecfd93d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature, config in feature_handling.items():\n",
    "    if config[\"is_selected\"] and config[\"feature_variable_type\"]==\"numerical\":\n",
    "        impute_values = config['feature_details']['missing_values']\n",
    "        if impute_values==\"Impute\":\n",
    "            impute_with=config[\"feature_details\"][\"impute_with\"]\n",
    "            if impute_with==\"Average of values\":\n",
    "                df[feature]=df[feature].fillna(df[feature].mean)\n",
    "            elif impute_with==\"Median\":\n",
    "                df[feature]=df[feature].fillna(df[feature].median)\n",
    "            elif impute_with==\"Most frequent values\":\n",
    "                df[feature]=df[feature].fillna(df[feature].mode)\n",
    "            elif impute_with==\"custom\":\n",
    "                df[feature]=df[feature].fillna(config[\"feature_details\"][\"impute_value\"])\n",
    "            else:\n",
    "                print(f\"Warning: {impute_with} is not supported by feature {feature}. Imputation is skipped\")\n",
    "        elif impute_values==\"keep as is\":\n",
    "            pass\n",
    "        elif impute_values==\"Drop feature\":\n",
    "            df=df.drop(columns=[feature],errors=\"ignore\")\n",
    "        \n",
    "    elif config[\"is_selected\"] and config[\"feature_variable_type\"]==\"text\":\n",
    "        impute_values = config['feature_details']['text_handling']\n",
    "        hash_columns=config['feature_details']['hash_columns']\n",
    "        if impute_values == 'Tokenize and hash': #and hash_columns==0:\n",
    "            df[feature],a=pd.factorize(df[feature])\n",
    "        else:\n",
    "            print(\"Encoding cannot be done\")\n",
    "    else:\n",
    "        print(f\"Warning: {impute_values} not supported for feature {feature}. Handling missing value is skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990c5139-abe6-41a9-a0eb-6d3e52095bc0",
   "metadata": {},
   "source": [
    "# Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e341d63d-279a-42eb-99a8-32ac7489c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_reduction=json_data[\"design_state_data\"][\"feature_reduction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5adc3f99-d7ae-46c1-87c1-dc7541c089a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_with_target_reduction(df,target_column,final_num_feature):\n",
    "    corrs = df.corr()[target_column].abs().sort_values(ascending=False)\n",
    "    top_features = corrs.head(final_num_feature+1).index.tolist()\n",
    "    return df[top_features]\n",
    "def tree_based_reduction(df,target_column,final_num_feature,num_of_trees,depth_of_trees):\n",
    "    if final_num_feature<df.shape[1]:\n",
    "        try:\n",
    "            x=df.drop(columns=[target_column],errors=\"ignore\")\n",
    "            y=df[target_column]\n",
    "            model=RandomForestRegressor(n_estimators=num_of_trees, max_depth=depth_of_trees, random_state=0)\n",
    "            model.fit(x,y)\n",
    "            sel = SelectFromModel(model, max_features=final_num_feature)\n",
    "            x_red = sel.fit_transform(x,y)\n",
    "            top_features=x.columns[sel.get_support()]\n",
    "            return df[[target_column]+list(top_features)]\n",
    "        except KeyError:\n",
    "            print(\"Target column not found skipping tree-based reduction\")\n",
    "            return df\n",
    "    else:\n",
    "        return df\n",
    "def PCA_reduction(df,target_column,final_num_feature):\n",
    "    if final_num_feature < df.shape[1] -1:\n",
    "        try:\n",
    "            x=df.drop(columns=[target_column],errors=\"ignore\")\n",
    "            y=df[target_column]\n",
    "            model=PCA(n_components=final_num_feature)\n",
    "            x_reduced=model.fit_transform(x)\n",
    "            pc_cols=[f\"PC{i+1}\" for i in range(x_reduced.shape[1])]\n",
    "            x_reduced_df = pd.DataFrame(x_reduced, columns=pc_cols, index=df.index)\n",
    "            return pd.concat([x_reduced_df, y], axis=1)\n",
    "        except KeyError:\n",
    "            print(\"Target Column not found skipping PCA reduction\")\n",
    "            return df\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "reduction_method=feature_reduction[\"feature_reduction_method\"]\n",
    "final_num_feature=int(feature_reduction['num_of_features_to_keep'])-1\n",
    "if reduction_method==\"Corr with Target\":\n",
    "    df=corr_with_target_reduction(df.copy(),target_column,final_num_feature)\n",
    "elif reduction_method==\"Tree-based\":\n",
    "    num_of_trees=int(feature_reduction['num_of_trees'])\n",
    "    depth_of_trees=int(feature_reduction['depth_of_trees'])\n",
    "    df=tree_based_reduction(df.copy(),target_column,final_num_feature,num_of_trees,depth_of_trees)\n",
    "elif reduction_method==\"PCA\":\n",
    "    df=PCA_reduction(df.copy(),target_column,final_num_feature)\n",
    "else:\n",
    "    print(f\"Warning: Feature reduction method '{reduction_method}' not supported. Skipping feature reduction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec552c12-140f-4990-9aff-3ca6dbfda221",
   "metadata": {},
   "source": [
    "# Model making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "938b409a-da5b-47a5-be89-91a6b390a146",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms_config=json_data[\"design_state_data\"][\"algorithms\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bda96d53-40a1-4561-89a6-f5b380f32ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier is not selected. Skipping.\n",
      "Random Forest Regressor\n",
      "Fitting 6 folds for each of 396 candidates, totalling 2376 fits\n",
      "Best parameters:{'max_depth': 20, 'min_samples_leaf': 8, 'n_estimators': 18}\n",
      "mse = 0.04737531527594438\n",
      "r2s = 0.9249064503390991\n",
      "mae = 0.16763632807942688\n",
      "Gradient Boosted Trees is not selected. Skipping.\n",
      "Gradient Boosted Trees is not selected. Skipping.\n",
      "LinearRegression is not selected. Skipping.\n",
      "LogisticRegression is not selected. Skipping.\n",
      "RidgeRegression is not selected. Skipping.\n",
      "Lasso Regression is not selected. Skipping.\n",
      "Lasso Regression is not selected. Skipping.\n",
      "XG Boost is not selected. Skipping.\n",
      "Decision Tree is not selected. Skipping.\n",
      "Decision Tree is not selected. Skipping.\n",
      "Support Vector Machine is not selected. Skipping.\n",
      "Stochastic Gradient Descent is not selected. Skipping.\n",
      "KNN is not selected. Skipping.\n",
      "Extra Random Trees is not selected. Skipping.\n",
      "Neural Network is not selected. Skipping.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def model_training_eval(df,algo_config,prediction_type,target_column,algo_name):\n",
    "    n_jobs=-1\n",
    "    if not algo_config['is_selected']:\n",
    "        print(f\"{algo_config['model_name']} is not selected. Skipping.\")\n",
    "        return\n",
    "    x=df.drop(columns=target_column,errors=\"ignore\")\n",
    "    y=df[target_column]\n",
    "    for col in x.select_dtypes(include=np.number).columns:\n",
    "        x[col] = x[col].astype(float)\n",
    "    if y.dtype != float: # Ensure target is also float\n",
    "        y = y.astype(float)\n",
    "    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=35)\n",
    "    mod_name=algo_config[\"model_name\"]\n",
    "    print(f\"{mod_name}\")\n",
    "    if pred_type == \"Regression\":\n",
    "        #we cant control number of iterations in linearRegression explicitly therefore we use SGDRegressor\n",
    "        if mod_name == \"LinearRegression\" and algo_name==\"LinearRegression\":\n",
    "            model=SGDRegressor()\n",
    "            #max_iter will take all possible integres given in range\n",
    "            #regparam, elasticnet are closely related with alpha and l1_ratio respectively\n",
    "            param_grid={\n",
    "                \"max_iter\": list(np.arange(algo_config[\"min_iter\"],algo_config[\"max_iter\"]+1)),\n",
    "                \"alpha\": list(np.arange(algo_config[\"min_regparam\"],algo_config[\"max_regparam\"]+0.01,0.1)),\n",
    "                \"l1_ratio\": list(np.arange(algo_config[\"min_elasticnet\"],algo_config[\"max_elasticnet\"]+0.01,0.1))\n",
    "            }\n",
    "            #parallelism=0 => n_jobs=-1(uses all cpu cores)\n",
    "            #parallelism=n_jobs for value greater than 0\n",
    "            #we cant set n_jobs=0\n",
    "            if algo_config[\"parallelism\"] == 0:\n",
    "                n_jobs=-1\n",
    "            else:\n",
    "                n_jobs=algo_config[\"parallelism\"]\n",
    "            \n",
    "            \"\"\"\n",
    "            model=LinearRegression()\n",
    "            param_grid={\n",
    "                \"fit_intercept\":[True,False],\n",
    "                \"positive\":[True,False]\n",
    "            }\n",
    "            \"\"\"\n",
    "            \n",
    "        elif mod_name==\"Random Forest Regressor\" and algo_name==\"RandomForestRegressor\":\n",
    "            model = RandomForestRegressor(random_state=42)\n",
    "            #number of trees and samples_per_leaf are corrosponds to n_estimator and min_sample_leaf\n",
    "            #max_feature = 1 is assumed as default feature_sampling_strategy\n",
    "            param_grid={\n",
    "                \"n_estimators\":list(np.arange(algo_config[\"min_trees\"],algo_config[\"max_trees\"]+1)),\n",
    "                \"max_depth\":list(np.arange(algo_config[\"min_depth\"],algo_config[\"max_depth\"]+1)),\n",
    "                \"min_samples_leaf\":list(np.arange(algo_config[\"min_samples_per_leaf_min_value\"],algo_config[\"min_samples_per_leaf_max_value\"]+1)),\n",
    "            }\n",
    "            #parallelism=0 => n_jobs=-1(uses all cpu cores)\n",
    "            #parallelism=n_jobs for value greater than 0\n",
    "            #we cant set n_jobs=0\n",
    "            if algo_config[\"parallelism\"] == 0:\n",
    "                n_jobs=-1\n",
    "            else:\n",
    "                n_jobs=algo_config[\"parallelism\"]\n",
    "        elif mod_name==\"Gradient Boosted Trees\" and algo_name==\"GBTRegressor\":\n",
    "            model = GradientBoostingRegressor(random_state=42)\n",
    "            #n_estimator => number of BoostingStages\n",
    "            #step_size is learning_rate\n",
    "            #spliting sample => subsample range is either (0.0,1.0] or [2,inf)  for min_sibsample=1 sample cannot be split further\n",
    "            #\"feature_sampling_statergy\": \"Fixed number\"=> GBTRegressor does not explicitly map this parameter to max_features in its param_grid\n",
    "            #\"use_deviance\": True, \"use_exponential\": False => GBTRegressor does not explicitly map this parameter to max_features in its param_grid\n",
    "            #\"fixed_number\": 22 => GBTRegressor does not explicitly map this parameter to max_features in its param_grid\n",
    "            #learning_rate => stepsize\n",
    "            param_grid={\n",
    "                \"n_estimators\": (algo_config[\"num_of_BoostingStages\"]),\n",
    "                \"learning_rate\": list(np.arange(algo_config[\"min_stepsize\"],algo_config[\"max_stepsize\"]+0.01,0.01)),\n",
    "                #\"min_samples_split\":list(np.arange(algo_config[\"min_subsample\"],algo_config[\"max_subsample\"]+1)),\n",
    "                \"max_depth\": list(np.arange(algo_config[\"min_depth\"],algo_config[\"max_depth\"]+1))\n",
    "            }\n",
    "        elif mod_name == \"RidgeRegression\" and algo_name==\"RidgeRegression\":\n",
    "            model = Ridge()\n",
    "            # you intend to provide specific values or a range for the regularization parameter.\n",
    "            #alpha=>regparam\n",
    "            param_grid = {\n",
    "                \"alpha\": list(np.arange(algo_config[\"min_regparam\"],algo_config[\"max_regparam\"]+0.01,0.1)),\n",
    "                \"max_iter\":list(np.arange(algo_config[\"min_iter\"],algo_config[\"max_iter\"]+1)),\n",
    "            }\n",
    "        elif mod_name == 'Lasso Regression' and algo_name==\"LassoRegression\":\n",
    "            model = Lasso()\n",
    "            # you intend to provide specific values or a range for the regularization parameter.\n",
    "            #alpha=>regparam\n",
    "            param_grid = {\n",
    "                \"max_iter\": list(np.arange(algo_config[\"min_iter\"],algo_config[\"max_iter\"]+1)),\n",
    "                \"alpha\": list(np.arange(algo_config[\"min_regparam\"],algo_config[\"max_regparam\"]+0.01,0.1)),\n",
    "                \"selection\": [\"cyclic\", \"random\"],\n",
    "            }\n",
    "        elif algo_name==\"ElasticNetRegression\":\n",
    "            model = ElasticNet()\n",
    "            print(\"a\")\n",
    "            param_grid = {\n",
    "                \"alpha\": list(np.arange(algo_config[\"min_regparam\"],algo_config[\"max_regparam\"]+0.01,0.1)),\n",
    "                \"l1_ratio\": list(np.arange(algo_config[\"min_elasticnet\"],algo_config[\"max_elasticnet\"]+0.01,0.1)),\n",
    "                \"max_iter\": list(np.arange(algo_config[\"min_iter\"],algo_config[\"max_iter\"]+1))\n",
    "            }\n",
    "        #There is no gini, entropy criterion for DecisionTreeRegressor\n",
    "        elif mod_name == 'Decision Tree' and algo_name==\"DecisionTreeRegressor\":   \n",
    "            model = DecisionTreeRegressor(random_state=42)\n",
    "            #\"use_best\": True, \"use_random\": True: => Decision Tree does not explicitly map these parameters to the splitter\n",
    "            splitter=[]\n",
    "            if algo_config[\"use_best\"]:\n",
    "                splitter.append(\"best\")\n",
    "            if algo_config[\"use_random\"]:\n",
    "                splitter.append(\"random\")\n",
    "            param_grid = {\n",
    "                \"max_depth\": list(np.arange(algo_config[\"min_depth\"],algo_config[\"max_depth\"]+1)),\n",
    "                \"min_samples_leaf\": algo_config[\"min_samples_per_leaf\"],\n",
    "                \"criterion\": [\"squared_error\", \"friedman_mse\", \"absolute_error\"],\n",
    "                \"splitter\": splitter\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Warning: Algorithm '{mod_name}' not supported for regression. Skipping.\")\n",
    "            return\n",
    "    else:\n",
    "        print(f\"Warning: Prediction type '{prediction_type}' not supported. Skipping.\")\n",
    "        return\n",
    "    pipeline=Pipeline([(\"scaler\",StandardScaler),\n",
    "                      (\"model\",model)])\n",
    "    num_of_folds=json_data[\"design_state_data\"][\"hyperparameters\"][\"num_of_folds\"]\n",
    "    n_jobs=json_data[\"design_state_data\"][\"hyperparameters\"][\"parallelism\"]\n",
    "    grid_search_cv=GridSearchCV(model,param_grid=param_grid,cv=num_of_folds,scoring=\"neg_mean_squared_error\",verbose=3,n_jobs=n_jobs)\n",
    "    grid_search_cv.fit(x_train,y_train)\n",
    "    best_est=grid_search_cv.best_estimator_\n",
    "    best_params=grid_search_cv.best_params_\n",
    "    y_pred=best_est.predict(x_test)\n",
    "    mse=mean_squared_error(y_test,y_pred)\n",
    "    r2s=r2_score(y_test,y_pred)\n",
    "    mae=mean_absolute_error(y_test,y_pred)\n",
    "    print(f\"Best parameters:{best_params}\")\n",
    "    print(f\"mse = {mse}\")\n",
    "    print(f\"r2s = {r2s}\")\n",
    "    print(f\"mae = {mae}\")\n",
    "for algo_name,algo_config in algorithms_config.items():\n",
    "    #passing data, prediction type,target, algorithm name and its configuration\n",
    "    model_training_eval(df.copy(),algo_config,pred_type,target_column,algo_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
